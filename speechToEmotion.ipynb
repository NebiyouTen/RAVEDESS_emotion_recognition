{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import numpy as np\n",
    "import os\n",
    "import torch \n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "stdout = sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "# emotions\n",
    "emotion= [\"neutral\",\"calm\",\"happy\", \"sad\",\"angry\",\"fearful\", \"disgust\",\"surprised\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_zero_mean(x):\n",
    "    \n",
    "    mean=((x.sum(axis=2)) / x.shape[2])\n",
    "    \n",
    "    zero_mean = x - mean.reshape(mean.shape[0],mean.shape[1],1)\n",
    "    return zero_mean\n",
    "    pass\n",
    "\n",
    "def feature_zero_mean(x):\n",
    "    \n",
    "    mean=((x.sum(axis=1)) / x.shape[1])\n",
    "    \n",
    "    zero_mean = x - mean.reshape(mean.shape[0],1)\n",
    "    return zero_mean\n",
    "    pass\n",
    "\n",
    "def data_zero_mean(x):\n",
    "    \n",
    "    mean=((x.sum(axis=0)) / x.shape[0])\n",
    "    \n",
    "    zero_mean = x - mean.reshape(1,mean.shape[0])\n",
    "    return zero_mean\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "dir_n = \"data/train_data/happy-sad-neutral/t2/\"\n",
    "files = np.array(os.listdir(\"data/train_data/happy-sad-neutral/t2/\"))\n",
    "for i,name in enumerate(files):\n",
    "    print ('Tranforming ',i,2100,len(files))\n",
    "    data_list.append(name)\n",
    "    print (str(dir_n)+str(name))\n",
    "    sound = AudioSegment.from_wav(str(dir_n)+\"/\"+str(name))\n",
    "    sound = sound.set_channels(1)\n",
    "    sound.export(str(dir_n)+\"/\"+str(name), format=\"wav\")\n",
    "file_names = np.array(data_list).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyAudioAnalysis import audioBasicIO\n",
    "from pyAudioAnalysis import audioFeatureExtraction\n",
    "\n",
    "def get_items(batch):\n",
    "    label= []\n",
    "    data_list = []\n",
    "    \n",
    "    for i in batch:\n",
    "#         y, sr = librosa.load(\"data/train_data/happy-sad-neutral/t2/\"+file_names[i],duration=3,offset=1.0)\n",
    "#         y = librosa.util.normalize(y)\n",
    "#         S = librosa.feature.melspectrogram(y, sr=sr, n_mels=128)\n",
    "#         log_S = librosa.power_to_db(S, ref=np.max)\n",
    "        temp_label = float(file_names[i][7])\n",
    "        \n",
    "        if temp_label in [1]:\n",
    "            label.append(float(0))\n",
    "        elif temp_label in [3]:\n",
    "            label.append(float(1))\n",
    "        else:\n",
    "            label.append(float(2))\n",
    "#         mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "#         tonnetz = librosa.effects.harmonic(y=y )\n",
    "#         tonnetz = librosa.feature.tonnetz(y=tonnetz, sr=sr)\n",
    "#         rms = librosa.feature.rmse(y=y)\n",
    "#         chroma_cqt = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "# #         feature_zero_mean(data_zero_mean(mfccs))\n",
    "#         abc = feature_zero_mean(data_zero_mean(np.concatenate((mfccs,\n",
    "#                               S,\n",
    "#                               tonnetz,\n",
    "#                               rms,\n",
    "#                               chroma_cqt\n",
    "#                              ),axis=0)))\n",
    "# #         print (abc.shape)\n",
    "\n",
    "\n",
    "        [Fs, x] = audioBasicIO.readAudioFile(\"data/train_data/happy-sad-neutral/t2/\"+file_names[i]);\n",
    "        F = audioFeatureExtraction.stFeatureExtraction(x, Fs, 0.050*Fs, 0.025*Fs);\n",
    "        abc = feature_zero_mean(data_zero_mean(F))\n",
    "        abc = np.transpose(abc,(1,0))\n",
    "        data_list.append(abc)\n",
    "        import sys\n",
    "        reload(sys)\n",
    "        sys.setdefaultencoding('utf-8')\n",
    "        sys.stdout = stdout\n",
    "    return np.array(data_list),np.array([label]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_use_shared_memory = False\n",
    "def collate (batch):\n",
    "    train ,in_label = get_items(batch)\n",
    "    \n",
    "#     print (in_label.shape)\n",
    "    N=train.shape[0]\n",
    "    max_seq = train[0].shape[0]\n",
    "    \n",
    "    seq_lens=torch.IntTensor(N)\n",
    "    \n",
    "    for i in np.arange(N):\n",
    "        if(max_seq<train[i].shape[0]):\n",
    "            max_seq=train[i].shape[0]\n",
    "        seq_lens[i]=train[i].shape[0]\n",
    "#         print('Train ',train[i].shape)\n",
    "    L=max_seq\n",
    "    D=train[i].shape[1]\n",
    "    if _use_shared_memory:\n",
    "        tr = torch.FloatStorage._new_shared(max_seq*train.shape[0]*D).new().zero_()\n",
    "    else:\n",
    "        tr = torch.FloatTensor(N,L,D).zero_()\n",
    "        inp_lbl = torch.FloatTensor(N).zero_()\n",
    "    for i in np.arange(N):\n",
    "        tr[i, 0:train[i].shape[0] , : ]=to_tensor(train[i])\n",
    "        #inp_lbl[i] = to_tensor(in_label[i].reshape(-1,1)) \n",
    "    sort_len,ind = torch.sort(seq_lens,descending=True)\n",
    "    tr2=  tr[ind,:,:]\n",
    "    inp_lbl=to_tensor(in_label)\n",
    "    inp_lbl=inp_lbl[ind]\n",
    "    return tr2,sort_len,inp_lbl\n",
    "\n",
    "def to_tensor(numpy_array):\n",
    "    return torch.from_numpy(numpy_array).float()\n",
    "\n",
    "\n",
    "def to_variable(tensor):\n",
    "    if torch.cuda.is_available():\n",
    "        #print (\"making it cuda\")\n",
    "        tensor = tensor.cuda()\n",
    "    return torch.autograd.Variable(tensor,requires_grad=True)\n",
    "\n",
    "def to_variable2(tensor):\n",
    "    if torch.cuda.is_available():\n",
    "        #print (\"making it cuda\")\n",
    "        tensor = tensor.cuda()\n",
    "    return torch.autograd.Variable(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "data_loader = DataLoader(np.arange(len(file_names)), shuffle=True,\n",
    "                         batch_size=32, collate_fn=collate)\n",
    "for  batch_idx,(inputs, seqlen,targets) in enumerate(data_loader):\n",
    "    print (targets.view(1,-1).shape,inputs.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv1D, self).__init__()\n",
    "        self.output_size = 46\n",
    "        self.n_layers = 2\n",
    "        self.c1 = nn.Conv1d(34,128,3,padding=1)\n",
    "        self.c3 = nn.Conv1d(128,32,3,padding=1)\n",
    "        self.batchNorm = nn.BatchNorm1d(32)\n",
    "        self.linear = nn.Linear(32,3)\n",
    "        #print (\"relu added\")\n",
    "    def forward(self, inputs,lens):\n",
    "        mask = to_variable2(torch.zeros(inputs.shape[0],inputs.shape[1]))\n",
    "        for i in np.arange(len(lens)):\n",
    "            mask[i,0:lens[i]]=1\n",
    "            \n",
    "        inputs = torch.transpose(inputs,1,2)\n",
    "        out = F.leaky_relu(self.c1(inputs))\n",
    "        out = out * mask.unsqueeze(1) \n",
    "        out =  F.leaky_relu(self.c3(out))\n",
    "        out = out * mask.unsqueeze(1) \n",
    "        out = F.dropout((F.leaky_relu(out)),0.3)\n",
    "        out = self.batchNorm(out)\n",
    "        \n",
    "        out = torch.mean(out,2)\n",
    "        print(out.shape)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torch.nn as nn\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.baseLSTM1 = nn.LSTM(input_size=187, hidden_size=64,bidirectional=True,batch_first=True,num_layers=5)\n",
    "        self.bnorm = nn.BatchNorm1d(64*2)\n",
    "        self.projection = nn.Linear(in_features=64*2, out_features=3)\n",
    "    def forward(self, input,seq):\n",
    "        h=input\n",
    "        \n",
    "#         print (seq)\n",
    "        h = rnn.pack_padded_sequence(h, seq,batch_first=True)\n",
    "        h,_ = self.baseLSTM1(h)\n",
    "        h, lens = rnn.pad_packed_sequence(h,batch_first=True)\n",
    "        mask = torch.zeros(h.shape[0],h.shape[1])\n",
    "        for i in np.arange(len(lens)):\n",
    "            mask[i,0:lens[i]]=1\n",
    "        mask = torch.cat([mask.unsqueeze(2)]*h.shape[2],2)\n",
    "#         print (h.shape,mask.shape)\n",
    "#         h = h * to_variable2(mask)\n",
    "#             print (h.shape,lens)\n",
    "        out = h[:,-1,:]\n",
    "        \n",
    "        return F.leaky_relu(self.projection(self.bnorm(out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Conv1D()\n",
    "net.load_state_dict(torch.load('models/models_First_2300.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate= 5e-4\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)#,weight_decay=1e-5)\n",
    "net.train()\n",
    "train_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "for epoch in np.arange(300):\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for  batch_idx,(inputs, seqlen,targets) in enumerate(data_loader):\n",
    "        if batch_idx == len(data_loader)-1:\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "#         print (targets.view(1,-1))\n",
    "        inputs, targets = to_variable(inputs), to_variable2(targets)\n",
    "#         print ('asdf ',inputs.shape,len(seq_len.numpy()))\n",
    "        outputs = net(inputs,seqlen.numpy())\n",
    "        \n",
    "        #print (\"target is \",targets)\n",
    "#         print (targets.long().view(-1),targets.long().view(-1).shape)\n",
    "        loss = criterion(outputs, targets.long().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.data[0]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted = predicted.view(-1,1)\n",
    "#         print (predicted.view(1,-1))\n",
    "#         print (targets.view(1,-1))\n",
    "#         print (predicted.numpy().T)\n",
    "#         print (targets.int().data.numpy().T)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.long().data).cpu().sum()\n",
    "        if (batch_idx % 1 == 0):\n",
    "            print(batch_idx, len(data_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "#             print (confusion_matrix(targets.data.view(-1).long(),predicted.view(-1,1)))\n",
    "    \n",
    "            \n",
    "        #break\n",
    "        if (batch_idx % 15  == 0):\n",
    "            print (\"Model saving\")\n",
    "#             learning_rate=learning_rate/2\n",
    "            torch.save(net.state_dict(), \"models/models_First_2\"+str(epoch)+str(batch_idx)+\".pt\")\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
