{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "A\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "import librosa.display\n",
    "import numpy as np\n",
    "import os\n",
    "import torch \n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import librosa\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "from pyAudioAnalysis import audioBasicIO\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "from pyAudioAnalysis import audioBasicIO\n",
    "from pyAudioAnalysis import audioFeatureExtraction\n",
    "# emotions\n",
    "emotion= [\"neutral\",\"calm\",\"happy\", \"sad\",\"angry\",\"fearful\", \"disgust\",\"surprised\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_zero_mean(x):\n",
    "    \n",
    "    mean=((x.sum(axis=2)) / x.shape[2])\n",
    "    \n",
    "    zero_mean = x - mean.reshape(mean.shape[0],mean.shape[1],1)\n",
    "    return zero_mean\n",
    "    pass\n",
    "\n",
    "def feature_zero_mean(x):\n",
    "    \n",
    "    mean=((x.sum(axis=1)) / x.shape[1])\n",
    "    \n",
    "    zero_mean = x - mean.reshape(mean.shape[0],1)\n",
    "    return zero_mean\n",
    "    pass\n",
    "\n",
    "def data_zero_mean(x):\n",
    "    \n",
    "    mean=((x.sum(axis=0)) / x.shape[0])\n",
    "    \n",
    "    zero_mean = x - mean.reshape(1,mean.shape[0])\n",
    "    return zero_mean\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "dir_n = \"data/test_data/happy-sad-neutral/t2/\"\n",
    "files = np.array(os.listdir(\"data/test_data/happy-sad-neutral/t2/\"))\n",
    "from pydub import AudioSegment\n",
    "\n",
    "for i,name in enumerate(files):\n",
    "    data_list.append(name)\n",
    "    sound = AudioSegment.from_wav(str(dir_n)+\"/\"+str(name))\n",
    "    sound = sound.set_channels(1)\n",
    "    sound.export(str(dir_n)+\"/\"+str(name), format=\"wav\")\n",
    "file_names = np.array(data_list).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items(batch):\n",
    "    label= []\n",
    "    data_list = []\n",
    "    \n",
    "    for i in batch:\n",
    "#         y, sr = librosa.load(\"data/test_data/happy-sad-neutral/t2/\"+file_names[i],duration=3,offset=1.0)\n",
    "#         y = librosa.util.normalize(y)\n",
    "#         S = librosa.feature.melspectrogram(y, sr=sr, n_mels=128)\n",
    "#         log_S = librosa.power_to_db(S, ref=np.max)\n",
    "        temp_label = float(file_names[i][7])\n",
    "        \n",
    "#         print (float(file_names[i][7]))\n",
    "        if temp_label in [1]:\n",
    "            label.append(float(0))\n",
    "        elif temp_label in [3]:\n",
    "            label.append(float(1))\n",
    "        else:\n",
    "            label.append(float(2))\n",
    "#         mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "#         tonnetz = librosa.effects.harmonic(y=y )\n",
    "#         tonnetz = librosa.feature.tonnetz(y=tonnetz, sr=sr)\n",
    "#         rms = librosa.feature.rmse(y=y)\n",
    "#         chroma_cqt = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "# #         feature_zero_mean(data_zero_mean(mfccs))\n",
    "#         abc = feature_zero_mean(data_zero_mean(np.concatenate((mfccs,\n",
    "#                               S,\n",
    "#                               tonnetz,\n",
    "#                               rms,\n",
    "#                               chroma_cqt\n",
    "#                              ),axis=0)))\n",
    "#         print (abc.shape)\n",
    "        [Fs, x] = audioBasicIO.readAudioFile(\"data/test_data/happy-sad-neutral/t2/\"+file_names[i]);\n",
    "        F = audioFeatureExtraction.stFeatureExtraction(x, Fs, 0.050*Fs, 0.025*Fs);\n",
    "        abc = feature_zero_mean(data_zero_mean(F))\n",
    "        abc = np.transpose(abc,(1,0))\n",
    "        data_list.append(abc)\n",
    "        import sys\n",
    "        reload(sys)\n",
    "        sys.setdefaultencoding('utf-8')\n",
    "        sys.stdout = stdout\n",
    "\n",
    "#         abc = np.transpose(abc,(1,0))\n",
    "#         data_list.append(abc)\n",
    "    return np.array(data_list),np.array([label]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_use_shared_memory = False\n",
    "def collate (batch):\n",
    "    train ,in_label = get_items(batch)\n",
    "    \n",
    "#     print (in_label.shape)\n",
    "    N=train.shape[0]\n",
    "    max_seq = train[0].shape[0]\n",
    "    \n",
    "    seq_lens=torch.IntTensor(N)\n",
    "    \n",
    "    for i in np.arange(N):\n",
    "        if(max_seq<train[i].shape[0]):\n",
    "            max_seq=train[i].shape[0]\n",
    "        seq_lens[i]=train[i].shape[0]\n",
    "#         print('Train ',train[i].shape)\n",
    "    L=max_seq\n",
    "    D=train[i].shape[1]\n",
    "    if _use_shared_memory:\n",
    "        tr = torch.FloatStorage._new_shared(max_seq*train.shape[0]*D).new().zero_()\n",
    "    else:\n",
    "        tr = torch.FloatTensor(N,L,D).zero_()\n",
    "        inp_lbl = torch.FloatTensor(N).zero_()\n",
    "    for i in np.arange(N):\n",
    "        tr[i, 0:train[i].shape[0] , : ]=to_tensor(train[i])\n",
    "        #inp_lbl[i] = to_tensor(in_label[i].reshape(-1,1)) \n",
    "    sort_len,ind = torch.sort(seq_lens,descending=True)\n",
    "    tr2=  tr[ind,:,:]\n",
    "    inp_lbl=to_tensor(in_label)\n",
    "    inp_lbl=inp_lbl[ind]\n",
    "    return tr2,sort_len,inp_lbl\n",
    "\n",
    "def to_tensor(numpy_array):\n",
    "    return torch.from_numpy(numpy_array).float()\n",
    "\n",
    "\n",
    "def to_variable(tensor):\n",
    "    if torch.cuda.is_available():\n",
    "        #print (\"making it cuda\")\n",
    "        tensor = tensor.cuda()\n",
    "    return torch.autograd.Variable(tensor,requires_grad=True)\n",
    "\n",
    "def to_variable2(tensor):\n",
    "    if torch.cuda.is_available():\n",
    "        #print (\"making it cuda\")\n",
    "        tensor = tensor.cuda()\n",
    "    return torch.autograd.Variable(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "data_loader = DataLoader(np.arange(len(file_names)), shuffle=False,\n",
    "                         batch_size=32, collate_fn=collate)\n",
    "\n",
    "for i in data_loader:\n",
    "    print ('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv1D, self).__init__()\n",
    "        self.output_size = 46\n",
    "        self.n_layers = 2\n",
    "        self.c1 = nn.Conv1d(34,128,3,padding=1)\n",
    "        self.c3 = nn.Conv1d(128,32,3,padding=1)\n",
    "        self.batchNorm = nn.BatchNorm1d(32)\n",
    "        self.linear = nn.Linear(32,3)\n",
    "        #print (\"relu added\")\n",
    "    def forward(self, inputs,lens):\n",
    "        mask = to_variable2(torch.zeros(inputs.shape[0],inputs.shape[1]))\n",
    "        for i in np.arange(len(lens)):\n",
    "            mask[i,0:lens[i]]=1\n",
    "            \n",
    "        inputs = torch.transpose(inputs,1,2)\n",
    "        out = F.leaky_relu(self.c1(inputs))\n",
    "        out = out * mask.unsqueeze(1) \n",
    "        out =  F.leaky_relu(self.c3(out))\n",
    "        out = out * mask.unsqueeze(1) \n",
    "        out = F.dropout((F.leaky_relu(out)),0.3)\n",
    "        out = self.batchNorm(out)\n",
    "        \n",
    "        out = torch.mean(out,2)\n",
    "        print(out.shape)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Conv1D()\n",
    "net.load_state_dict(torch.load('models/model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32])\n",
      "(0, 3, 'Loss: 0.348 | Acc: 84.375% (27/32)')\n",
      "torch.Size([32, 32])\n",
      "(1, 3, 'Loss: 0.379 | Acc: 85.938% (55/64)')\n",
      "torch.Size([21, 32])\n",
      "(2, 3, 'Loss: 0.366 | Acc: 88.235% (75/85)')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net.eval()\n",
    "train_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "train_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "p=[]\n",
    "l=[]\n",
    "for  batch_idx,(inputs, seqlen,targets) in enumerate(data_loader):\n",
    "#     if batch_idx == len(data_loader)-1:\n",
    "#         continue\n",
    "    inputs, targets = to_variable(inputs), to_variable2(targets)\n",
    "    outputs = net(inputs,seqlen.numpy())\n",
    "    loss = criterion(outputs, targets.long().view(-1))\n",
    "    train_loss += loss.data[0]\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "#     print (np.round(F.softmax(outputs).data.numpy(),2))\n",
    "    predicted = predicted.view(-1,1)\n",
    "    if (batch_idx==0):\n",
    "        probs = \n",
    "        p=predicted.numpy()\n",
    "        l=targets.data.numpy()\n",
    "    else: \n",
    "        probs = np.vstack((probs,outputs.data.numpy()))\n",
    "        p=np.vstack((p,predicted.numpy()))\n",
    "        l=np.vstack((l,targets.data.numpy()))\n",
    "    total += targets.size(0)\n",
    "    correct += predicted.eq(targets.long().data).cpu().sum()\n",
    "    print(batch_idx, len(data_loader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_true = l# ground truth labels\n",
    "y_probas = # predicted probabilities generated by sklearn classifier\n",
    "skplt.metrics.plot_roc_curve(y_true, y_probas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 45   0  15]\n",
      " [  3  78   0]\n",
      " [  6   6 102]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print (confusion_matrix(l,p)*3)\n",
    "sum(sum(confusion_matrix(l,p))*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_model = Conv1D()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
